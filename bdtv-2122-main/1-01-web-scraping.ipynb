{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with _requests_ and _BeautifulSoup_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will practice web scraping and information extraction using the packages _requests_ and _BeautifulSoup_ and the world renowned website [www.p-tech.org.uk](https://www.p-tech.org.uk).\n",
    "\n",
    "To begin, if they are not already installed and up-to-date, we need to install the relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\python\\python310\\lib\\site-packages (2.27.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python310\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\python310\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python\\python310\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python310\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python\\python310\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\empjb\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user requests\n",
    "!pip install --user beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you have problems with SSL certificates further down the notebook you may need to update _requests._ You can do this with either:\n",
    "\n",
    "_!pip install --upgrade requests_\n",
    "\n",
    "_!pip install --upgrade --user requests_ &nbsp;&nbsp;&nbsp; _# if you don't have admin privileges_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can import our packages. We will also setup a header variable - this basically tells the website this request is from a normal browser-type agent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "\n",
    "headers = ({'User-Agent':\n",
    "            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can specify our website and make a GET request (as per the lecture). We can print the start of the response to check we have been able to access the site content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if IE 9 ]> <html lang=\"en-US\" class=\"ie9 loading-site no-js\"> <![endif]-->\n",
      "<!--[if IE 8 ]> <html lang=\"en-US\" class=\"ie8 loading-site no-js\"> <![endif]-->\n",
      "<!--[if (gte IE 9)|!(IE)]><!--><html lang=\"en-US\" class=\"loading-site no-js\"> <!--<![endif]-->\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<link rel=\"profile\" href=\"http://gmpg.org/xfn/11\"/>\n",
      "<link rel=\"pingback\" href=\"xmlrpc.php.html\"/>\n",
      "<script>(function(html){html.className = html.className.replace(/\\bno-js\\b/,'js')})(document.documen\n"
     ]
    }
   ],
   "source": [
    "ptech_web = \"http://ptechweb.s3-website.us-east-2.amazonaws.com\"\n",
    "response = get(ptech_web, headers=headers)\n",
    "\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have scraped some website content lets see if we can find anything useful in it (by parsing the content with _BeautifulSoup)._ I mean, its unlikely given the source material but we will persevere. Let's start by extracting all the _h1_ tags (first-level headers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1>Digital Marketing Consultancy</h1>, <h1>Paid Advertising Training</h1>, <h1>International Marketing</h1>, <h1>Google Analytics Training</h1>]\n"
     ]
    }
   ],
   "source": [
    "ptech_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "ptech_h1 = ptech_soup.find_all('h1')\n",
    "print(ptech_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something similar for _h4_ tags, but this time we will specify a particular CSS class ('thin-font') and extract only the text rather than the HTML tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to understand your online marketing?\n",
      "Google Adwords & Facebook Advertising\n",
      "Are you looking to Export?\n",
      "Learn how to reach new markets.\n",
      "Get started with Analytics, understand the data\n"
     ]
    }
   ],
   "source": [
    "ptech_tag = ptech_soup.find_all('h4', class_='thin-font')\n",
    "\n",
    "for each in ptech_tag: \n",
    "    print(str(each.get_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst we can see that the site has a strange approach to capitalisation, everything does seems to have worked! Let's try something a bit more complicated.\n",
    "\n",
    "Here we will loop between two web pages (the _suffixes_ list below) and extract the _h1_ headings, the _meta title_ and the _meta description._ As the website stores content inside [containers](https://www.w3schools.com/w3css/w3css_containers.asp), we will need to search inside these, again via a loop. Finally you will note we use the _time_ module in order to make the script wait - via _sleep( )._ This is good practice when scraping websites as a script can make a lot of requests very quickly and overload the website server. We sleep for a random time just for fun. We also time the whole process using the Notebook function _%%time._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "# setting up the lists/dictionary that will form our dataframe with all the results\n",
    "titles = []\n",
    "metatitle = []\n",
    "title_dict = {} # empty dictionary\n",
    "\n",
    "uri = 'http://ptechweb.s3-website.us-east-2.amazonaws.com/about-us/'\n",
    "suffixes = ['staff/james-pennington', 'clients'] # add the rest in here \n",
    "\n",
    "for suffix in suffixes: # loop through the suffixes list\n",
    "\n",
    "    h1heading = [] # new list or empty the list to start again \n",
    "    metatitle = []\n",
    "    metadesc = []\n",
    "    ptech_webscrape = uri + suffix # concatenate the URL and the suffix in the current loop  \n",
    "    r = get(ptech_webscrape, headers=headers)\n",
    "    page_html = BeautifulSoup(r.text, 'html.parser')\n",
    "    ptech_webpage = page_html.find_all('html')       \n",
    "    \n",
    "    if ptech_webpage != []:\n",
    "        for container in ptech_webpage:\n",
    "            \n",
    "            # page title\n",
    "            meta_title = page_html.find(\"meta\", property=\"og:title\")\n",
    "            metatitle.append(meta_title)\n",
    "            \n",
    "            # meta description\n",
    "            meta_desc = page_html.find(\"meta\", property=\"og:description\")\n",
    "            metadesc.append(meta_desc)\n",
    "            \n",
    "            # H1\n",
    "            h1name = container.find_all('h1')[0].text\n",
    "            h1heading.append(h1name)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    title_dict[suffix] = h1heading, metatitle, metadesc # add to the dictionary the suffix and the list of h1's\n",
    "    \n",
    "    \n",
    "    sleep(randint(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'staff/james-pennington': (['James Pennington'], [<meta content=\"James Pennington - ptech\" property=\"og:title\"/>], [<meta content=\"James has been involved in IT consultancy to SMEâs for 18+ years and has worked with over a large number of businesses in that time to help them understand and implement IT effectively within their organisations. With both a practical and strategic view of IT across all areas of the business, James works with a [...]\" property=\"og:description\"/>]), 'clients': (['Our Clients'], [<meta content=\"Our Clients - ptech\" property=\"og:title\"/>], [<meta content=\"We have delivered our services to a wide range of clients over the past 10 years, including business start-ups and Global organisations. Â What ever the type of business you’re running, we are confident that we can provided the skills and support required to take your business to the next level. Our clients, whether they be [...]\" property=\"og:description\"/>])}\n"
     ]
    }
   ],
   "source": [
    "print(title_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish off our work we will add our data to a _pandas_ DataFrame and export as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Title'] # columns in our output file\n",
    "\n",
    "ptechcsv = pd.DataFrame({'Title': title_dict})[cols]\n",
    "ptechcsv.to_csv('ptech_scrape.csv') # the name of our file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have scraped a website :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
